vocab_size: 32
padding_idx: 1

# 99M parameters (if no chain-aware attention)
# 124M parameters (if chain-aware attention)
d_model: 384
n_layers: 56
n_heads: 6
d_ffn: null
ffn_multiplier: null

max_seq_len: 320

dropout: 0.1
attention_dropout: 0.1
embedding_dropout: 0.1

use_chain_aware_attention: true

# Normalization options
norm_type: layernorm  # "layernorm" or "rmsnorm"
pre_norm: true
post_norm: false
qk_norm: none  # "none", "norm", or "learned_scale"
layer_norm_eps: 1e-6
