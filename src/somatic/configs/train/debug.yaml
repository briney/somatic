max_steps: 100
max_epochs: null

batch_size: 4
gradient_accumulation_steps: 1

optimizer:
  name: adamw
  learning_rate: 1e-4
  weight_decay: 0.01
  betas: [0.9, 0.999]

max_grad_norm: 1.0

scheduler:
  decay: constant
  warmup_steps: 10
  min_lr_ratio: 0.1

log_steps: 1
eval_steps: 50
checkpoint_steps: 50

checkpoint_dir: ${output_dir}/checkpoints
keep_last_n_checkpoints: 2
save_best: false

mixed_precision: "no"
